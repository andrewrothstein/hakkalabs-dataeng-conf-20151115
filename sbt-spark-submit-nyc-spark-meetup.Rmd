---
title: "sbt-spark-submit"
subtitle: "Straight-through Build to Spark Analytics"
output:
  #ioslides_presentation:
  bfmmarkdown::bfmslides_presentation:
    #self_contained: false
    css: 
      - stylesheets/mermaid.forest.css
      - stylesheets/meetup.css
    mathjax: null
date: "2015-09-24"
author:
  - name: "Forest Fang<br> Financial Modeling Group - Advanced Data Analytics"
---

## What People Think We Do {.centered}

<img src="http://s.tf1.fr/mmdia/i/67/6/minorityreport2-scan-17-3938676dxwep.jpg?v=1" width="100%" />
<footer class="source">https://99designs.com/designer-blog/2013/07/19/fantasy-user-interfaces/</footer>

## What We Really Do {.centered}

![](http://s.quickmeme.com/img/ac/ac49486fe031828370067a6988293e43bca97a15d08508e388f0a757a9700103.jpg)
<footer class="source">http://www.quickmeme.com/p/3vovpz</footer>

## Word Count {.centered}

<img src="http://xiaochongzhang.me/blog/wp-content/uploads/2013/05/MapReduce_Work_Structure.png" width="100%" />"
<footer class="source">http://www.milanor.net/blog/?p=853</footer>

## Word Count So Easy! {.smaller}

```java
import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.net.URI;
import java.util.ArrayList;
import java.util.HashSet;
import java.util.List;
import java.util.Set;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.Counter;
import org.apache.hadoop.util.GenericOptionsParser;
import org.apache.hadoop.util.StringUtils;
```
<footer class="source">http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Example:_WordCount_v2.0</footer>

## Word Count So Easy! {.smaller}

```java
public class WordCount2 {

  public static class TokenizerMapper
       extends Mapper<Object, Text, Text, IntWritable>{

    static enum CountersEnum { INPUT_WORDS }

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    private boolean caseSensitive;
    private Set<String> patternsToSkip = new HashSet<String>();

    private Configuration conf;
    private BufferedReader fis;

    @Override
    public void setup(Context context) throws IOException,
        InterruptedException {
      conf = context.getConfiguration();
      caseSensitive = conf.getBoolean("wordcount.case.sensitive", true);
      if (conf.getBoolean("wordcount.skip.patterns", true)) {
        URI[] patternsURIs = Job.getInstance(conf).getCacheFiles();
        for (URI patternsURI : patternsURIs) {
          Path patternsPath = new Path(patternsURI.getPath());
          String patternsFileName = patternsPath.getName().toString();
          parseSkipFile(patternsFileName);
        }
      }
    }
```

## Word Count So Easy! {.smaller}

```java

    private void parseSkipFile(String fileName) {
      try {
        fis = new BufferedReader(new FileReader(fileName));
        String pattern = null;
        while ((pattern = fis.readLine()) != null) {
          patternsToSkip.add(pattern);
        }
      } catch (IOException ioe) {
        System.err.println("Caught exception while parsing the cached file '"
            + StringUtils.stringifyException(ioe));
      }
    }

    @Override
    public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      String line = (caseSensitive) ?
          value.toString() : value.toString().toLowerCase();
      for (String pattern : patternsToSkip) {
        line = line.replaceAll(pattern, "");
      }
      StringTokenizer itr = new StringTokenizer(line);
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
        Counter counter = context.getCounter(CountersEnum.class.getName(),
            CountersEnum.INPUT_WORDS.toString());
        counter.increment(1);
      }
    }
  }

  public static class IntSumReducer
       extends Reducer<Text,IntWritable,Text,IntWritable> {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values,
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }
```

## Word Count So Easy! {.smaller}

```java
  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    GenericOptionsParser optionParser = new GenericOptionsParser(conf, args);
    String[] remainingArgs = optionParser.getRemainingArgs();
    if (!(remainingArgs.length != 2 | | remainingArgs.length != 4)) {
      System.err.println("Usage: wordcount <in> <out> [-skip skipPatternFile]");
      System.exit(2);
    }
    Job job = Job.getInstance(conf, "word count");
    job.setJarByClass(WordCount2.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);

    List<String> otherArgs = new ArrayList<String>();
    for (int i=0; i < remainingArgs.length; ++i) {
      if ("-skip".equals(remainingArgs[i])) {
        job.addCacheFile(new Path(remainingArgs[++i]).toUri());
        job.getConfiguration().setBoolean("wordcount.skip.patterns", true);
      } else {
        otherArgs.add(remainingArgs[i]);
      }
    }
    FileInputFormat.addInputPath(job, new Path(otherArgs.get(0)));
    FileOutputFormat.setOutputPath(job, new Path(otherArgs.get(1)));

    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}
```

## Word Count So Easy! {.centered}

<img src="https://brianadamspr.files.wordpress.com/2012/11/was-that-sarcasm.jpg" width="50%" />
<footer class="source">http://knowledgespeakswisdomlistens.blogspot.com/2013/01/am-i-sheldon-cooper.html</footer>

## We Do Big Data {.centered}

<img src="http://cdn.meme.am/instances2/500x/1902975.jpg" width="60%" />

## Until Spark Came Along

```scala
val textFile = spark.textFile("hdfs://...")
val counts = textFile.flatMap(line => line.split(" "))
                 .map(word => (word, 1))
                 .reduceByKey(_ + _)
counts.saveAsTextFile("hdfs://...")
```

### 

<div class="centered">
  <img src="http://spark.apache.org/images/logistic-regression.png" height="250px" />
  <img src="http://img.memecdn.com/happy-no-cat_o_1052698.jpg" height="250px" />
</div>

<footer class="source">http://www.dispuutcirce.nl/leden/img_2426/
http://spark.apache.org/</footer>

## Spark First World Problem {.centered}

![](https://i.imgflip.com/r4xnp.jpg)

## Submit Spark Application {.smaller}

Create application JAR
```
sbt vis/assembly
```

Upload JAR to cluster
```
scp vis/target/scala-2.10/vis-assembly-x.y.z-SNAPSHOT.jar my-awesome-spark-cluster:
```

<div class="centered">
  <img src="http://geekshumor.com/wp-content/uploads/2013/06/My-codes-compiling.png" width="50%" />
</div>

<footer class="source">https://xkcd.com/303/</footer>

## Submit Spark Application {.smaller}

The *spark-submit* script in Sparkâ€™s bin directory is used to launch applications on a cluster.

```bash
# Run application locally on 8 cores
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master local[8] \
  /path/to/examples.jar \
  100

# Run on a Spark Standalone cluster in client deploy mode
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://207.184.161.138:7077 \
  --executor-memory 20G \
  --total-executor-cores 100 \
  /path/to/examples.jar \
  1000
```

> - Commands difficult to remember
> - No easy way to store different pairs of arguments

<footer class="source">http://spark.apache.org/docs/latest/submitting-applications.html</footer>

## Submit Spark Application

```{r, echo=FALSE}
DiagrammeR::mermaid("
sequenceDiagram
  developer->>sbt: create assembly jar
  sbt->>developer: done packaging
  alt client mode
    developer->>cluster host: scp jar
    cluster host->>developer: done
    developer->>cluster host: spark-submit
    cluster host->>cluster manager: submit job synchronously
    cluster manager->>developer: done
  else cluster mode
    developer->>cluster manager: spark-submit asynchronously
    cluster manager->>developer: submitted and running
  end
", width = "800", height = "500")
```

<div class="notes">
The process is a bit tedious and because every step is not instantaneous I often find myself execute one step and only realize I should proceed with next step minutes later.
</div>



## Bash script rocks! {.smaller}

```bash
#!/bin/bash

PROJECT_VER=SNAPSHOT
PROJECT_ML_JAR=${PROJECT_DIR}/project-ml-assembly-${PROJECT_VER}.jar

PREFIX=kmeans
HBASE_KEYPREFIX=${PREFIX}_${SUFFIX}
HBASE_CLASSPATH=/etc/hbase/conf:/usr/lib/hbase/lib/hbase-client.jar:/usr/lib/hbase/lib/hbase-common.jar:/usr/lib/hbase/lib/hbase-examples.jar:/usr/lib/hbase/lib/hbase-hadoop2-compat.jar:/usr/lib/hbase/lib/hbase-hadoop-compat.jar:/usr/lib/hbase/lib/hbase-it.jar:/usr/lib/hbase/lib/hbase-prefix-tree.jar:/usr/lib/hbase/lib/hbase-protocol.jar:/usr/lib/hbase/lib/hbase-server.jar:/usr/lib/hbase/lib/hbase-shell.jar:/usr/lib/hbase/lib/hbase-testing-util.jar:/usr/lib/hbase/lib/hbase-thrift.jar:/usr/lib/hbase/lib/htrace-core-2.04.jar

OUTPUT_HDFS_DIR=${PREFIX}/${SUFFIX}

time \
    spark-submit \
    --master yarn-cluster \
    --num-executors 1000 \
    --class com.bfm.amps.KMClustering \
    --driver-class-path ${HBASE_CLASSPATH} \
    --driver-memory=3g --executor-memory=3g \
    --conf="spark.executor.extraClassPath=${HBASE_CLASSPATH}" \
    --conf="spark.yarn.jar=hdfs:///proj/share/spark/spark-assembly-1.5.0-hadoop2.6.0.jar" \
    ${PROJECT_ML_JAR} \
    -output=${OUTPUT_HDFS_DIR} -input=my_awesome_big_dataset \
    -k=10 -partitions=10 -runs=10 \
    -hbase=${HBASE_KEYPREFIX}
```

## Bash script rocks! {.centered}

![](http://cdn.meme.am/instances/64064889.jpg)
<footer class="source">http://memegenerator.net/instance/37544999</footer>

## Bash script rocks?

> - Environment isolation difficult to achieve (repeatability)
> - Weak abstraction and encapsulation (code reuse)
> - Runtime exception only (reliability)

<div class="centered">
  ![](http://images-cdn.moviepilot.com/image/upload/c_fill,h_281,t_mp_quality,w_500/-2a86a3c7-9206-41b1-9f39-558bbe914d27.gif)
</div>
<footer class="source">http://animatedmeme.blogspot.com/2013/06/peter-griffin-vs-blinds.html</footer>

## Scala! {.centered}

![](http://cdn.meme.am/instances/58038541.jpg)
<footer class="source">http://memegenerator.net/instance/58038541</footer>

# *sbt-spark-submit* Plugin to Rescue | Demo: SparkPi Submission for Local and YARN

<div class="notes">
change deploy strategy on the go;
change settings on the go;
can store multiple configuration;
alter configuration only triggers rebuild the build not the project;
works with multiple main classes
</div>

## What is *sbt*

> The interactive build tool
>
> Use Scala to define your tasks. Then run them in parallel from the shell.

> - *build.sbt* uses an elegant Scala DSL for build definition 
> - *project/\*.scala* use full power of Scala to configure the build and create custom tasks

<footer class="source">http://www.scala-sbt.org/</footer>

## Why *sbt*

> - Fast incremental build
> - Continous compilation/testing
> - Concise and powerful configuration
> - Built-in auto-complete
> - IDE highlights compile time exception

## Scala! {.centered}

<img src="https://s-media-cache-ak0.pinimg.com/736x/7b/b7/06/7bb706cc6f2ac6e62c58b299acf3f362.jpg" width="50%" />
<footer class="source">http://www.iamprogrammer.net/</footer>

## Scala! {.centered}

<img src="http://i.imgur.com/ju1CgAo.jpg" width="50%" />
<footer class="source">http://imgur.com/r/programmingmemes/ju1CgAo</footer>

# Cats Don't Like Dogs | Demo: A Multi-project Build

<div class="notes">
task is bind to specific project

automatic common assembly target deduplication;
automatic parallel execution
</div>

## *sbt* Multi-project and Deduplication

> - Create tasks specific to each project
> - Common settings/tasks are automatically deduplicated
> - Parallelizable tasks are automatically executed concurrently

# It's extensible! | Demo: Deploy on *AWS EC2*

<div class="notes">
https://github.com/saurfang/sbt-spark-submit/tree/master/examples/sbt-assembly-on-ec2

We can add `libraryDependencies` in `plugins.sbt` which adds the dependencies into the build not the project.
Now we have all the Java and Scala libraries at our disposable to create our build.

*InputSetting* are overriable and can contain side effects that prepare our deployment.
</div>

## *sbt-spark-submit* Recap

> - Streamline build, upload and submit into single task
> - Codify *spark-submit* command into settings
> - Integrate well for multi-project
> - Extensible

## Thank you {.smaller}

Slides and code

https://github.com/saurfang/nyc-spark-meetup

*sbt-spark-submit*

https://github.com/saurfang/sbt-spark-submit

<div class="centered">
  ![](http://www.blankchapters.com/wp-content/uploads/2012/12/meme-data-data-everywhere.png)
</div>

<footer class="source">http://www.blankchapters.com/2012/12/22/big-data-is-not-always-the-solution-to-all-our-problems</footer>

# Appendix

## Spark Cluster Mode Overview

> - Driver Program
> - Executors (worker, slave, container)

<div class="centered">
  ![](http://spark.apache.org/docs/latest/img/cluster-overview.png)
</div>

<footer class="source">http://spark.apache.org/docs/latest/cluster-overview.html</footer>

<div class="notes">
Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program).

Driver Program -> Communicate with Cluster Manager to request resources. Broadcast JAR and common files to executors. Sends serialized tasks to executors to run.
Executors -> Listen to Driver for tasks to process and respond with status. Inter-executor communication to exchange shuffled/broadcasted data.
</div>

## Client Deployment Mode {.centered}

![](http://www.cloudera.com/content/cloudera/en/documentation/core/latest/images/spark-yarn-client.png)

<footer class="source">http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cdh_ig_running_spark_on_yarn.html</footer>


<div class="notes">
Client mode runs the Driver Program in the same JVM as *spark-submit*. This has the benefit that logs are shown on the same console. However since Driver Program constantly communicates with Executors, it's often required to run it on a co-located cluster host.
The reality is that Developer often doesn't develops in the same data center as the Hadoop cluster. Wouldn't it be nice if we can submit to a remote cluster right here from our development environment? This is where cluster deployment mode comes in!
</div>


## Cluster Deployment Mode {.centered}

<div class="centered">
  ![](http://www.cloudera.com/content/cloudera/en/documentation/core/latest/images/spark-yarn-cluster.png)
</div>

<footer class="source">http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cdh_ig_running_spark_on_yarn.html</footer>


<div class="notes">
Cluster mode runs the Driver Program in a container JVM. For the case of YARN, it runs in the same container where Application Master lives. This ensures driver and executers can communicate with low latency.

While logs now can only be retrieved as application logs in YARN, this has the added benefits that logs are automatically archived.

Let's see how this simplifies things!
</div>

## The Role of Developer (Cluster Mode)

> - Create application JAR
    ```
    sbt vis/assembly
    ```
    Include all runtime dependencies
> - Execute *spark-submit*
    ```
    export HADOOP_CONF_DIR=<yarn-site.xml location>
    $SPARK_HOME/bin/spark-submit --class my.DataVis \
      --master yarn-cluster --num-executors 100 \
      vis/target/scala-2.10/vis-assembly-x.y.z-SNAPSHOT.jar \
      --input test_input --output test_output --test myMagicalTestingFeature
    ```
> - Developer has to set up `$SPARK_HOME` with correct version. Not perfectly repeatable


<div class="notes">
We are happy to see we eliminated one step! However now developer is required to setup the SPARK distribution on their dev machine. It's not like we need an extra step for scare new developer away and not too mention this can be a bit brittle when you bump Spark version.
</div>
