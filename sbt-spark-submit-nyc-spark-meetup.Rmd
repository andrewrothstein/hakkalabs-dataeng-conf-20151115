---
title: "sbt-spark-submit"
subtitle: "Straight-through Build to Spark Analytics"
output:
  #ioslides_presentation:
  bfmmarkdown::bfmslides_presentation:
    #self_contained: false
    css: 
      - stylesheets/mermaid.forest.css
      - stylesheets/meetup.css
    mathjax: null
date: "2015-09-24"
author:
  - name: "Forest Fang<br> Financial Modeling Group - Advanced Data Analytics"
---

## Agenda

> - Spark deployment architecture
> - Development workflow and challenges
> - Introduction to *sbt-spark-submit*
> - What is *sbt*?
> - Continous develop and deploy to *AWS EC2*

<div class="notes">
- Application Driver Program and Executors
- The role of developer and challenge it introduces
- sbt plugin that achieves straight-through build, upload, spark-submit
with accessible configuration
- incremental build, multi-project routing, extensible, deduplicate
- example to extend sbt-spark-submit to add additional deployment steps
</div>

## Spark Cluster Mode Overview

> - Driver Program
> - Executors (worker, slave, container)

<div class="centered">
  ![](http://spark.apache.org/docs/latest/img/cluster-overview.png)
</div>

<footer class="source">http://spark.apache.org/docs/latest/cluster-overview.html</footer>

<div class="notes">
Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program).

Driver Program -> Communicate with Cluster Manager to request resources. Broadcast JAR and common files to executors. Sends serialized tasks to executors to run.
Executors -> Listen to Driver for tasks to process and respond with status. Inter-executor communication to exchange shuffled/broadcasted data.
</div>

## The Role of User {.smaller}

The *spark-submit* script in Sparkâ€™s bin directory is used to launch applications on a cluster.

```bash
# Run application locally on 8 cores
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master local[8] \
  /path/to/examples.jar \
  100

# Run on a Spark Standalone cluster in client deploy mode
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://207.184.161.138:7077 \
  --executor-memory 20G \
  --total-executor-cores 100 \
  /path/to/examples.jar \
  1000
```

> - Commands difficult to remember
> - No easy way to store different pairs of arguments

<footer class="source">http://spark.apache.org/docs/latest/submitting-applications.html</footer>


## The Role of Developer

> - Create application JAR
    ```
    sbt vis/assembly
    ```
    Include all runtime dependencies
> - Upload JAR to co-located cluster host
    ```
    scp vis/target/scala-2.10/vis-assembly-x.y.z-SNAPSHOT.jar \
      my-awesome-spark-cluster:
    ```
> - Execute *spark-submit*
    ```
    export HADOOP_CONF_DIR=<yarn-site.xml location>
    $SPARK_HOME/bin/spark-submit --class my.DataVis --master yarn \
      --num-executors 100 vis-assembly-x.y.z-SNAPSHOT.jar \
      --input test_input --output test_output --test myMagicalTestingFeature
    ```

<div class="notes">
The process is a bit tedious and because every step is not instantaneous I often find myself execute one step and only realize I should proceed with next step minutes later.
</div>

## The Role of Developer {.centered}

![](http://geekshumor.com/wp-content/uploads/2013/06/My-codes-compiling.png)

## Client Deployment Mode {.centered}

![](http://www.cloudera.com/content/cloudera/en/documentation/core/latest/images/spark-yarn-client.png)

<footer class="source">http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cdh_ig_running_spark_on_yarn.html</footer>


<div class="notes">
Client mode runs the Driver Program in the same JVM as *spark-submit*. This has the benefit that logs are shown on the same console. However since Driver Program constantly communicates with Executors, it's often required to run it on a co-located cluster host.
The reality is that Developer often doesn't develops in the same data center as the Hadoop cluster. Wouldn't it be nice if we can submit to a remote cluster right here from our development environment? This is where cluster deployment mode comes in!
</div>


## Cluster Deployment Mode {.centered}

<div class="centered">
  ![](http://www.cloudera.com/content/cloudera/en/documentation/core/latest/images/spark-yarn-cluster.png)
</div>

<footer class="source">http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cdh_ig_running_spark_on_yarn.html</footer>


<div class="notes">
Cluster mode runs the Driver Program in a container JVM. For the case of YARN, it runs in the same container where Application Master lives. This ensures driver and executers can communicate with low latency.

While logs now can only be retrieved as application logs in YARN, this has the added benefits that logs are automatically archived.

Let's see how this simplifies things!
</div>

## The Role of Developer (Cluster Mode)

> - Create application JAR
    ```
    sbt vis/assembly
    ```
    Include all runtime dependencies
> - Execute *spark-submit*
    ```
    export HADOOP_CONF_DIR=<yarn-site.xml location>
    $SPARK_HOME/bin/spark-submit --class my.DataVis \
      --master yarn-cluster --num-executors 100 \
      vis/target/scala-2.10/vis-assembly-x.y.z-SNAPSHOT.jar \
      --input test_input --output test_output --test myMagicalTestingFeature
    ```
> - Developer has to set up `$SPARK_HOME` with correct version. Not perfectly repeatable


<div class="notes">
We are happy to see we eliminated one step! However now developer is required to setup the SPARK distribution on their dev machine. It's not like we need an extra step for scare new developer away and not too mention this can be a bit brittle when you bump Spark version.
</div>

## The Role of Developer

```{r, echo=FALSE}
DiagrammeR::mermaid("
sequenceDiagram
  developer->>sbt: create assembly jar
  sbt->>developer: done packaging
  alt client mode
    developer->>cluster host: scp jar
    cluster host->>developer: done
    developer->>cluster host: spark-submit
    cluster host->>cluster manager: submit job synchronously
    cluster manager->>developer: done
  else cluster mode
    developer->>cluster manager: spark-submit asynchronously
    cluster manager->>developer: submitted and running
  end
", width = "800", height = "500")
```

## Spark Development Challenges

> - Multiple non-instantaneous steps
    - Build, Upload, Submit
> - Multiple main classes
    - Exploratory Analysis, Model Training, Model Evaluation
> - Multiple projects
    - Batch ETL, ML, Streaming
> - Multiple configurations
    - Different Datasets
> - Multiple deployment locations
    - local, yarn, EC2

## Bash script rocks! {.centered}

![](http://cdn.meme.am/instances/64064889.jpg)

## Bash script rocks?

> - Environment isolation difficult to achieve (repeatability)
> - Weak abstraction and encapsulation (code reuse)
> - Runtime exception only (reliability)

<div class="centered">
  ![](http://images-cdn.moviepilot.com/image/upload/c_fill,h_281,t_mp_quality,w_500/-2a86a3c7-9206-41b1-9f39-558bbe914d27.gif)
</div>


# *sbt-spark-submit* Plugin to Rescue | Demo: SparkPi Submission for Local and YARN

<div class="notes">
change deploy strategy on the go;
change settings on the go;
can store multiple configuration;
alter configuration only triggers rebuild the build not the project;
works with multiple main classes
</div>

## What is *sbt*

> The interactive build tool
>
> Use Scala to define your tasks. Then run them in parallel from the shell.

> - *build.sbt* uses an elegant Scala DSL for build definition 
> - *project/\*.scala* use full power of Scala to configure the build and create custom tasks

<footer class="source">http://www.scala-sbt.org/</footer>

## Why *sbt*

> - Fast incremental build
> - Continous compilation/testing
> - Concise and powerful configuration
> - Built-in auto-complete

# Cats Don't Like Dogs | Demo: A Multi-project Build

<div class="notes">
task is bind to specific project

automatic common assembly target deduplication;
automatic parallel execution
</div>

## *sbt* Multi-project and Deduplication

> - Create tasks specific to each project
> - Common settings/tasks are automatically deduplicated
> - Parallelizable tasks are automatically executed concurrently

# It's extensible! | Demo: Deploy on *AWS EC2*

<div class="notes">
https://github.com/saurfang/sbt-spark-submit/tree/master/examples/sbt-assembly-on-ec2

We can add `libraryDependencies` in `plugins.sbt` which adds the dependencies into the build not the project.
Now we have all the Java and Scala libraries at our disposable to create our build.

*InputSetting* are overriable and can contain side effects that prepare our deployment.
</div>

## *sbt-spark-submit* Recap

> - Streamline build, upload and submit into single task
> - Codify *spark-submit* command into settings
> - Integrate well for multi-project
> - Extensible

## Thank you

### Slides and code

https://github.com/saurfang/nyc-spark-meetup

### *sbt-spark-submit*

https://github.com/saurfang/sbt-spark-submit

